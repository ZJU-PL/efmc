
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>efmc.llmtools.llm_utils &#8212; EFMC Documentation</title>
    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/classic.css" />
    
    <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../../_static/doctools.js"></script>
    
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 
  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="../../../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="nav-item nav-item-0"><a href="../../../index.html">EFMC Documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="../../index.html" accesskey="U">Module code</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">efmc.llmtools.llm_utils</a></li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <h1>Source code for efmc.llmtools.llm_utils</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;LLM utilities for online inference using various LLM providers.&quot;&quot;&quot;</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">concurrent.futures</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Tuple</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">tiktoken</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">openai</span><span class="w"> </span><span class="kn">import</span> <span class="n">OpenAI</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">efmc.llmtools.logger</span><span class="w"> </span><span class="kn">import</span> <span class="n">Logger</span>


<div class="viewcode-block" id="LLM"><a class="viewcode-back" href="../../../llmtools.html#efmc.llmtools.llm_utils.LLM">[docs]</a><span class="k">class</span><span class="w"> </span><span class="nc">LLM</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    An online inference model using different LLMs:</span>
<span class="sd">    - DeepSeek: V3, R1 (uses OpenAI-compatible API)</span>
<span class="sd">    - GLM: Zhipu AI models</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">online_model_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">logger</span><span class="p">:</span> <span class="n">Logger</span><span class="p">,</span>
        <span class="n">temperature</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
        <span class="n">system_role</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="p">(</span>
            <span class="s2">&quot;You are a experienced programmer and good at understanding &quot;</span>
            <span class="s2">&quot;programs written in mainstream programming languages.&quot;</span>
        <span class="p">),</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initialize LLM instance.</span>

<span class="sd">        Args:</span>
<span class="sd">            online_model_name: Name of the online model</span>
<span class="sd">            logger: Logger instance</span>
<span class="sd">            temperature: Temperature for inference</span>
<span class="sd">            system_role: System role message</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">online_model_name</span> <span class="o">=</span> <span class="n">online_model_name</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoding</span> <span class="o">=</span> <span class="n">tiktoken</span><span class="o">.</span><span class="n">encoding_for_model</span><span class="p">(</span>
            <span class="s2">&quot;gpt-3.5-turbo-0125&quot;</span>
        <span class="p">)</span>  <span class="c1"># We only use gpt-3.5 to measure token cost</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">temperature</span> <span class="o">=</span> <span class="n">temperature</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">system_role</span> <span class="o">=</span> <span class="n">system_role</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span> <span class="o">=</span> <span class="n">logger</span>

<div class="viewcode-block" id="LLM.infer"><a class="viewcode-back" href="../../../llmtools.html#efmc.llmtools.llm_utils.LLM.infer">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">infer</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">message</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">is_measure_cost</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Infer using the online model.</span>

<span class="sd">        Args:</span>
<span class="sd">            message: Input message</span>
<span class="sd">            is_measure_cost: Whether to measure token cost</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tuple of (output, input_token_cost, output_token_cost)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">print_log</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">online_model_name</span><span class="p">,</span> <span class="s2">&quot;is running&quot;</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
        <span class="k">if</span> <span class="s2">&quot;deepseek&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">online_model_name</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">infer_with_deepseek_model</span><span class="p">(</span><span class="n">message</span><span class="p">)</span>
        <span class="k">elif</span> <span class="s2">&quot;glm&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">online_model_name</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">infer_with_glm_model</span><span class="p">(</span><span class="n">message</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Unsupported model name. Only DeepSeek and GLM models are supported.&quot;</span>
            <span class="p">)</span>

        <span class="n">input_token_cost</span> <span class="o">=</span> <span class="p">(</span>
            <span class="mi">0</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">is_measure_cost</span>
            <span class="k">else</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">encoding</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">system_role</span><span class="p">))</span>
            <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">encoding</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">message</span><span class="p">))</span>
        <span class="p">)</span>
        <span class="n">output_token_cost</span> <span class="o">=</span> <span class="p">(</span>
            <span class="mi">0</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">is_measure_cost</span> <span class="k">else</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">encoding</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">output</span><span class="p">))</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">input_token_cost</span><span class="p">,</span> <span class="n">output_token_cost</span></div>

<div class="viewcode-block" id="LLM.run_with_timeout"><a class="viewcode-back" href="../../../llmtools.html#efmc.llmtools.llm_utils.LLM.run_with_timeout">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">run_with_timeout</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">func</span><span class="p">,</span> <span class="n">timeout_seconds</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Run a function with timeout that works in multiple threads.</span>

<span class="sd">        Args:</span>
<span class="sd">            func: Function to execute</span>
<span class="sd">            timeout_seconds: Timeout in seconds</span>

<span class="sd">        Returns:</span>
<span class="sd">            Function result or empty string on timeout/error</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">with</span> <span class="n">concurrent</span><span class="o">.</span><span class="n">futures</span><span class="o">.</span><span class="n">ThreadPoolExecutor</span><span class="p">(</span><span class="n">max_workers</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="k">as</span> <span class="n">executor</span><span class="p">:</span>
            <span class="n">future</span> <span class="o">=</span> <span class="n">executor</span><span class="o">.</span><span class="n">submit</span><span class="p">(</span><span class="n">func</span><span class="p">)</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">future</span><span class="o">.</span><span class="n">result</span><span class="p">(</span><span class="n">timeout</span><span class="o">=</span><span class="n">timeout_seconds</span><span class="p">)</span>
            <span class="k">except</span> <span class="n">concurrent</span><span class="o">.</span><span class="n">futures</span><span class="o">.</span><span class="n">TimeoutError</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">print_log</span><span class="p">(</span><span class="s2">&quot;Operation timed out&quot;</span><span class="p">)</span>
                <span class="k">return</span> <span class="s2">&quot;&quot;</span>
            <span class="k">except</span> <span class="p">(</span><span class="ne">ValueError</span><span class="p">,</span> <span class="ne">RuntimeError</span><span class="p">,</span> <span class="ne">ConnectionError</span><span class="p">)</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">print_log</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Operation failed: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="k">return</span> <span class="s2">&quot;&quot;</span></div>

<div class="viewcode-block" id="LLM.infer_with_deepseek_model"><a class="viewcode-back" href="../../../llmtools.html#efmc.llmtools.llm_utils.LLM.infer_with_deepseek_model">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">infer_with_deepseek_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">message</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Infer using the DeepSeek model (V3, R1, etc.)</span>
<span class="sd">        DeepSeek uses OpenAI-compatible API format</span>

<span class="sd">        Args:</span>
<span class="sd">            message: Input message</span>

<span class="sd">        Returns:</span>
<span class="sd">            Model output or empty string on failure</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">api_key</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;DEEPSEEK_API_KEY&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">api_key</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">print_log</span><span class="p">(</span><span class="s2">&quot;DeepSeek API key not found in environment variables&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="s2">&quot;&quot;</span>

        <span class="n">model_input</span> <span class="o">=</span> <span class="p">[</span>
            <span class="p">{</span>
                <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span>
                <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">system_role</span><span class="p">,</span>
            <span class="p">},</span>
            <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">message</span><span class="p">},</span>
        <span class="p">]</span>

        <span class="k">def</span><span class="w"> </span><span class="nf">call_api</span><span class="p">():</span>
            <span class="n">client</span> <span class="o">=</span> <span class="n">OpenAI</span><span class="p">(</span><span class="n">api_key</span><span class="o">=</span><span class="n">api_key</span><span class="p">,</span> <span class="n">base_url</span><span class="o">=</span><span class="s2">&quot;https://api.deepseek.com/v1&quot;</span><span class="p">)</span>
            <span class="n">response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
                <span class="n">model</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">online_model_name</span><span class="p">,</span>
                <span class="n">messages</span><span class="o">=</span><span class="n">model_input</span><span class="p">,</span>
                <span class="n">temperature</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">temperature</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="n">response</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">content</span>

        <span class="n">try_count</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">while</span> <span class="n">try_count</span> <span class="o">&lt;</span> <span class="mi">5</span><span class="p">:</span>
            <span class="n">try_count</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">run_with_timeout</span><span class="p">(</span><span class="n">call_api</span><span class="p">,</span> <span class="n">timeout_seconds</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">output</span><span class="p">:</span>
                    <span class="k">return</span> <span class="n">output</span>
            <span class="k">except</span> <span class="p">(</span><span class="ne">ValueError</span><span class="p">,</span> <span class="ne">RuntimeError</span><span class="p">,</span> <span class="ne">ConnectionError</span><span class="p">)</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">print_log</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;DeepSeek API error: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>

        <span class="k">return</span> <span class="s2">&quot;&quot;</span></div>

<div class="viewcode-block" id="LLM.infer_with_glm_model"><a class="viewcode-back" href="../../../llmtools.html#efmc.llmtools.llm_utils.LLM.infer_with_glm_model">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">infer_with_glm_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">message</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Infer using the GLM model.</span>

<span class="sd">        Args:</span>
<span class="sd">            message: Input message</span>

<span class="sd">        Returns:</span>
<span class="sd">            Model output or empty string on failure</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="kn">from</span><span class="w"> </span><span class="nn">zhipuai</span><span class="w"> </span><span class="kn">import</span> <span class="n">ZhipuAI</span>  <span class="c1"># pylint: disable=import-outside-toplevel</span>
        <span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">print_log</span><span class="p">(</span>
                <span class="s2">&quot;zhipuai package not installed. Please install it to use GLM models.&quot;</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="s2">&quot;&quot;</span>

        <span class="n">api_key</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;GLM_API_KEY&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">api_key</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">print_log</span><span class="p">(</span><span class="s2">&quot;GLM API key not found in environment variables&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="s2">&quot;&quot;</span>

        <span class="n">model_input</span> <span class="o">=</span> <span class="p">[</span>
            <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">system_role</span><span class="p">},</span>
            <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">message</span><span class="p">},</span>
        <span class="p">]</span>

        <span class="k">def</span><span class="w"> </span><span class="nf">call_api</span><span class="p">():</span>
            <span class="n">client</span> <span class="o">=</span> <span class="n">ZhipuAI</span><span class="p">(</span><span class="n">api_key</span><span class="o">=</span><span class="n">api_key</span><span class="p">)</span>
            <span class="n">response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
                <span class="n">model</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">online_model_name</span><span class="p">,</span>
                <span class="n">messages</span><span class="o">=</span><span class="n">model_input</span><span class="p">,</span>
                <span class="n">temperature</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">temperature</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="n">response</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">content</span>

        <span class="n">try_count</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">while</span> <span class="n">try_count</span> <span class="o">&lt;</span> <span class="mi">5</span><span class="p">:</span>
            <span class="n">try_count</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">run_with_timeout</span><span class="p">(</span><span class="n">call_api</span><span class="p">,</span> <span class="n">timeout_seconds</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">output</span><span class="p">:</span>
                    <span class="k">return</span> <span class="n">output</span>
            <span class="k">except</span> <span class="p">(</span><span class="ne">ValueError</span><span class="p">,</span> <span class="ne">RuntimeError</span><span class="p">,</span> <span class="ne">ConnectionError</span><span class="p">)</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">print_log</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;API error: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>

        <span class="k">return</span> <span class="s2">&quot;&quot;</span></div></div>
</pre></div>

            <div class="clearer"></div>
          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="../../../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="nav-item nav-item-0"><a href="../../../index.html">EFMC Documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="../../index.html" >Module code</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">efmc.llmtools.llm_utils</a></li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2024, rainoftime.
      Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 5.0.0.
    </div>
  </body>
</html>